# Sistema de Embeddings en Segundo Plano

## Descripci√≥n

Este sistema genera autom√°ticamente embeddings vectoriales para todos los items almacenados en la base de datos, permitiendo b√∫squeda sem√°ntica y RAG (Retrieval-Augmented Generation).

## Arquitectura

### Componentes

1. **`utils/embeddings.py`**: Utilidades para generar embeddings
   - Modelo: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensiones)
   - Chunking: Divide textos largos en fragmentos de ~500 caracteres con overlap de 50
   - Proceso as√≠ncrono con thread pool para operaciones CPU-bound

2. **`database/embedding_dao.py`**: Acceso a datos de embeddings
   - `create()`: Almacena embedding chunk con upsert
   - `get_items_without_embeddings()`: Encuentra items pendientes de procesar
   - `search_similar()`: B√∫squeda por similitud coseno usando pgvector

3. **Background Worker en `main.py`**:
   - Se inicia autom√°ticamente con el servidor FastAPI
   - Procesa 5 items por iteraci√≥n cada 10-30 segundos
   - Pre-carga el modelo de embeddings al inicio
   - Manejo de errores y cancelaci√≥n limpia

## Flujo de Procesamiento

```
[Item Creado] ‚Üí [Extraer Texto] ‚Üí [Cola de Procesamiento]
                                          ‚Üì
                                   [Background Worker]
                                          ‚Üì
                            [Chunking + Generate Embeddings]
                                          ‚Üì
                              [Almacenar en tabla embeddings]
                                          ‚Üì
                            [Disponible para b√∫squeda RAG]
```

## Uso

### Verificar Estado

```bash
curl http://localhost:5000/api/v1/embeddings/status
```

Respuesta:
```json
{
  "worker_running": true,
  "items_pending": 3,
  "model_loaded": true
}
```

### Generar Embeddings Manualmente (C√≥digo)

```python
from utils.embeddings import generate_embeddings_for_text

text = "Tu texto largo aqu√≠..."
embeddings = await generate_embeddings_for_text(text)

# Resultado: [(chunk_text, embedding_vector), ...]
for chunk, vector in embeddings:
    print(f"Chunk: {chunk[:50]}...")
    print(f"Vector dim: {len(vector)}")  # 384
```

### B√∫squeda Sem√°ntica

```python
from database.embedding_dao import EmbeddingDAO
from utils.embeddings import generate_embeddings_for_text, get_embedding_model

# Generar embedding de la consulta
model = get_embedding_model()
query_text = "C√≥mo mejorar la productividad"
query_embeddings = await generate_embeddings_for_text(query_text, model)
query_vector = query_embeddings[0][1]  # Primer chunk

# Buscar similares
dao = EmbeddingDAO(db.pool)
results = await dao.search_similar(query_vector, limit=5)

for result in results:
    print(f"Similitud: {result['similarity']:.3f}")
    print(f"Texto: {result['chunk_text']}")
    print(f"Item: {result['title']}")
```

## Configuraci√≥n

### Ajustar Tama√±o de Chunks

En `utils/embeddings.py`:

```python
chunks = chunk_text(text, max_chunk_size=500, overlap=50)
```

- `max_chunk_size`: Caracteres m√°ximos por chunk (default: 500)
- `overlap`: Caracteres de solapamiento entre chunks (default: 50)

### Ajustar Frecuencia del Worker

En `main.py` ‚Üí `_embedding_background_worker()`:

```python
await asyncio.sleep(10)  # Espera entre iteraciones
```

### Cambiar Modelo de Embeddings

En `utils/embeddings.py`:

```python
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
```

**Importante**: Si cambias el modelo, actualiza la dimensi√≥n en `init.sql`:

```sql
embedding vector(384)  -- Cambiar seg√∫n dimensi√≥n del modelo
```

## Requisitos

- `sentence-transformers>=5.0.0`
- `torch>=2.0.0` (versi√≥n CPU incluida para ahorrar espacio)
- PostgreSQL con extensi√≥n `pgvector`

## Monitoreo

El worker imprime logs en la salida est√°ndar:

```
‚úì Embedding background worker started
üìä Found 3 items to process for embeddings
üîÆ Generating embeddings for item: Clean Code Principles...
‚úì Stored 5 embeddings for item: Clean Code Principles
```

## Troubleshooting

### El worker no procesa items

1. Verificar que los items tengan `extracted_text` no vac√≠o
2. Verificar estado: `curl .../embeddings/status`
3. Revisar logs del servidor

### Error de dimensi√≥n en vectores

Aseg√∫rate de que la tabla `embeddings` tenga la dimensi√≥n correcta:

```sql
ALTER TABLE embeddings 
ALTER COLUMN embedding TYPE vector(384);  -- O la dimensi√≥n de tu modelo
```

### El modelo no se carga

Verifica que `sentence-transformers` est√© instalado:

```bash
uv pip install sentence-transformers
```

La primera vez descargar√° el modelo (~90MB), tarda unos segundos.

## Pr√≥ximos Pasos

1. **Implementar endpoint de b√∫squeda sem√°ntica**: `GET /api/v1/items/search-semantic`
2. **Integrar RAG en el chat**: Usar embeddings para recuperar contexto relevante
3. **Reindexaci√≥n**: Endpoint para regenerar embeddings de items existentes
4. **M√©tricas**: Tracking de tiempo de procesamiento y calidad de embeddings
